{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RedHerring10/Project1/blob/main/Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZIe42Epvjzc"
      },
      "outputs": [],
      "source": [
        "!pip -q install --upgrade transformers accelerate torch gradio sentencepiece safetensors"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Imports & Model Choice\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch, os, time\n",
        "import gradio as gr\n",
        "\n",
        "# Pick a SMALL instruct/chat model (change this string if you like):\n",
        "# Good CPU-friendly picks:\n",
        "#   - \"Qwen/Qwen2.5-0.5B-Instruct\"       (tiny, supports chat templates)\n",
        "#   - \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "MODEL_ID = \"your model here\"\n",
        "\n",
        "# Device setup (GPU if available)\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    dtype = torch.float16\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "    dtype = torch.float16\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    dtype = torch.float32\n",
        "\n",
        "tokenizer = \"-\"\n",
        "model = \"_\"\n",
        "if device == \"cpu\":\n",
        "    model = model.to(device)\n",
        "\n",
        "print(f\"Loaded {MODEL_ID} on {device}.\")"
      ],
      "metadata": {
        "id": "Xs_KLRLovpO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AX_HISTORY = 6        # keep last N turns\n",
        "MAX_NEW_TOKENS = 256   # generation length\n",
        "TEMPERATURE = 0.7\n",
        "TOP_P = 0.95\n",
        "\n",
        "def generate_reply(history):\n",
        "    \"\"\"history is a list of [user, assistant] pairs from Gradio Chatbot.\"\"\"\n",
        "    # Convert to messages format expected by chat templates\n",
        "    messages = []\n",
        "    for user_msg, bot_msg in history[-MAX_HISTORY:]:\n",
        "        messages.append({\"role\": \"user\", \"content\": user_msg})\n",
        "        if bot_msg is not None:\n",
        "            messages.append({\"role\": \"assistant\", \"content\": bot_msg})\n",
        "    # Add the latest user message (Gradio will pass it in the last pair)\n",
        "    if not messages or messages[-1][\"role\"] != \"user\" and history:\n",
        "        last_user = history[-1][0]\n",
        "        messages.append({\"role\": \"user\", \"content\": last_user})\n",
        "\n",
        "\n",
        "    # Build model input via chat template if available\n",
        "    try:\n",
        "        input_ids = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "    except Exception:\n",
        "        # Fallback: simple concatenation if template missing\n",
        "        prompt = \"\\n\".join([f\"User: {m['content']}\" if m[\"role\"]==\"user\" else f\"Assistant: {m['content']}\" for m in messages])\n",
        "        prompt += \"\\nAssistant:\"\n",
        "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    input_ids = input_ids.to(model.device)\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_new_tokens=MAX_NEW_TOKENS,\n",
        "            do_sample=True,\n",
        "            temperature=TEMPERATURE,\n",
        "            top_p=TOP_P,\n",
        "            pad_token_id=tokenizer.eos_token_id if tokenizer.eos_token_id is not None else tokenizer.pad_token_id,\n",
        "        )\n",
        "\n",
        "    # Slice off the prompt, decode only the new tokens if possible\n",
        "    gen_ids = outputs[0][input_ids.shape[-1]:]\n",
        "    text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
        "\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "9texoiKMvsjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Wire up a tiny Gradio UI (one component, one function)\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"\"\"## ðŸ¤– Tiny Chatbot\\nType below. The model runs locally on your machine.\"\"\")\n",
        "    chat = gr.Chatbot(height=400)\n",
        "    msg = gr.Textbox(placeholder=\"Ask me anything...\", label=\"Your message\")\n",
        "    clear = gr.Button(\"Clear chat\")\n",
        "\n",
        "    def user_submit(user_message, history):\n",
        "        if not history:\n",
        "            history = []\n",
        "        history.append([user_message, None])\n",
        "        return \"\", history\n",
        "\n",
        "    def bot_reply(history):\n",
        "        reply = generate_reply(history)\n",
        "        history[-1][1] = reply\n",
        "        return history\n",
        "\n",
        "    msg.submit(user_submit, [msg, chat], [msg, chat], queue=False).then(\n",
        "        bot_reply, inputs=[chat], outputs=[chat]\n",
        "    )\n",
        "    clear.click(lambda: [], outputs=[chat], queue=False)\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "t1j_43rRvx-c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}